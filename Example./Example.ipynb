{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47eb8c30-6eda-4389-bd1e-2b793623519c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nr/3vs7wdfj1mn1333tl23g4vy00000gn/T/ipykernel_61266/245859763.py:7: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython.display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    }
   ],
   "source": [
    "# ==== Standard Library ====\n",
    "from typing import List, Optional, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "import json\n",
    "import getpass\n",
    "import os\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# ==== Third-party Libraries ====\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "\n",
    "# ==== LangChain Core ====\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnableConfig\n",
    "from langchain_core.runnables import chain as as_runnable\n",
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage, AnyMessage\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# ==== LangChain Community ====\n",
    "from langchain_community.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper\n",
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "from langchain_community.vectorstores import InMemoryVectorStore\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "\n",
    "# ==== LangChain OpenAI ====\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# ==== LangGraph ====\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.pregel import RetryPolicy\n",
    "\n",
    "# ==== Local Modules ====\n",
    "from Utils  import *\n",
    "from Setup import *\n",
    "from Promt_tem import *\n",
    "from Gen_Initial_Outline import *\n",
    "from Expand_Topics import * \n",
    "from Gen_Perspectives import *\n",
    "from Interview_State import InterviewState\n",
    "from Dialog_Roles import *\n",
    "from search_engine import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4281789-ecd8-45db-b60c-4a5752541151",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Input #######\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"TAVILY_API_KEY\"] =\"\"\n",
    "\n",
    "_input = \"How People Are Really Using Gen AI in 2025\"\n",
    "\n",
    "selector = LLMSelector(\n",
    "        fast_model_name=\"gpt-4o\",\n",
    "        long_context_model_name=\"gpt-4.5-preview-2025-02-27\"\n",
    "    )\n",
    "use_long_context =   False\n",
    "fast_llm, long_context_llm = selector.get_llms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4437c856-b243-497b-8499-35946e3b233e",
   "metadata": {},
   "source": [
    "# Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a34fbf6-534e-4cbc-ac03-0848cd5bce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "\n",
    "\n",
    "class Tagging(BaseModel):\n",
    "    \"\"\"Tag the piece of text with particular info.\"\"\"\n",
    "    Topic: str = Field(description=\"Head line Topic \")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world-class writer, your job is to take a topic and create a new interesting topic that gets read.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "model_with_functions = fast_llm.bind(\n",
    "    functions=[convert_to_openai_function(Tagging)],\n",
    "    function_call={\"name\": \"Tagging\"}\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "tagging_chain = prompt | model_with_functions | JsonOutputFunctionsParser()\n",
    "aa = tagging_chain.invoke({\"input\":_input }) \n",
    "_input = aa['Topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6b4db09-35d5-4e10-ae37-e6c3915adddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Storm/lib/python3.12/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /opt/anaconda3/envs/Storm/lib/python3.12/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "## Generate Initial Outline\n",
    "gen = GenInitialOutline(fast_llm, long_context_llm)\n",
    "\n",
    "# outline\n",
    "initial_outline = gen.generate_outline(example_topic= _input , use_long_context=use_long_context)\n",
    "generate_outline_direct = direct_gen_outline_prompt | fast_llm.with_structured_output(\n",
    "    Outline\n",
    ")\n",
    "\n",
    "initial_outline = generate_outline_direct.invoke({\"topic\": _input})\n",
    "# initial_outline.as_str\n",
    "\n",
    "related_subjects, expand_chain = await expand_topics(_input, fast_llm)\n",
    "\n",
    "gen = GenPerspectives(fast_llm=fast_llm, expand_chain=expand_chain)\n",
    "perspectives = await gen.survey_subjects(_input)\n",
    "#perspectives.model_dump()\n",
    "#gen_perspectives_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d9d73ff-de30-436b-9ea3-cd64d4e0316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@as_runnable\n",
    "async def generate_question(state: InterviewState):\n",
    "    editor = state[\"editor\"]\n",
    "    gn_chain = (\n",
    "        RunnableLambda(swap_roles).bind(name=editor.name)\n",
    "        | gen_qn_prompt.partial(persona=editor.persona)\n",
    "        | fast_llm\n",
    "        | RunnableLambda(tag_with_name).bind(name=editor.name)\n",
    "    )\n",
    "    result = await gn_chain.ainvoke(state)\n",
    "    return {\"messages\": [result]}\n",
    "    \n",
    "messages = [\n",
    "    HumanMessage(f\"So you said you were writing an article on {_input}?\")\n",
    "]\n",
    "question = await generate_question.ainvoke(\n",
    "    {\n",
    "        \"editor\": perspectives.editors[0],\n",
    "        \"messages\": messages,\n",
    "    }\n",
    ")\n",
    "\n",
    "# question[\"messages\"][0].content\n",
    "#### Answer questions\n",
    "gen_queries_chain = gen_queries_prompt | fast_llm.with_structured_output(\n",
    "    Queries, include_raw=True, method=\"function_calling\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d08a15b6-88ee-47bb-a5f9-6f8bcc28ba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = await gen_queries_chain.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=question[\"messages\"][0].content)]}\n",
    ")\n",
    "# queries[\"parsed\"].queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c1a03e2-9480-462c-9c83-e46d58c7be54",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_answer_chain = gen_answer_prompt | fast_llm.with_structured_output(\n",
    "    AnswerWithCitations, include_raw=True\n",
    ").with_config(run_name=\"GenerateAnswer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d2b73f6-33b7-4ff9-9d33-bc8c2a28b78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# DDG\n",
    "search_engine = DuckDuckGoSearchAPIWrapper()\n",
    "\n",
    "@tool\n",
    "async def search_engine(query: str):\n",
    "    \"\"\"Search engine to the internet.\"\"\"\n",
    "    results = DuckDuckGoSearchAPIWrapper()._ddgs_text(query)\n",
    "    return [{\"content\": r[\"body\"], \"url\": r[\"href\"]} for r in results]\n",
    "'''\n",
    "\n",
    "# Tavily is typically a better search engine, but your free queries are limited\n",
    "search_engine = TavilySearchResults(max_results=4)\n",
    "tavily_search =  TavilySearchResults(\n",
    "    max_results=20,\n",
    "    include_answer=True,\n",
    "    include_raw_content=True,\n",
    "    include_images=True,\n",
    "    # search_depth=\"advanced\",\n",
    "    # include_domains = []\n",
    "    # exclude_domains = []\n",
    ")\n",
    "@tool\n",
    "async def search_engine(query: str):\n",
    "    \"\"\"Search engine to the internet.\"\"\"\n",
    "    results = tavily_search.invoke(query)\n",
    "    return [{\"content\": r[\"content\"], \"url\": r[\"url\"]} for r in results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0c8b967-dd18-4bd6-83bf-033e33dde018",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def gen_answer(\n",
    "    state: InterviewState,\n",
    "    config: Optional[RunnableConfig] = None,\n",
    "    name: str = \"Subject_Matter_Expert\",\n",
    "    max_str_len: int = 15000,\n",
    "):\n",
    "    swapped_state = swap_roles(state, name)  # Convert all other AI messages\n",
    "    queries = await gen_queries_chain.ainvoke(swapped_state)\n",
    "    query_results = await search_engine.abatch(\n",
    "        queries[\"parsed\"].queries, config, return_exceptions=True\n",
    "    )\n",
    "    successful_results = [\n",
    "        res for res in query_results if not isinstance(res, Exception)\n",
    "    ]\n",
    "    all_query_results = {\n",
    "        res[\"url\"]: res[\"content\"] for results in successful_results for res in results\n",
    "    }\n",
    "    # We could be more precise about handling max token length if we wanted to here\n",
    "    dumped = json.dumps(all_query_results)[:max_str_len]\n",
    "    ai_message: AIMessage = queries[\"raw\"]\n",
    "    tool_call = queries[\"raw\"].tool_calls[0]\n",
    "    tool_id = tool_call[\"id\"]\n",
    "    tool_message = ToolMessage(tool_call_id=tool_id, content=dumped)\n",
    "    swapped_state[\"messages\"].extend([ai_message, tool_message])\n",
    "    # Only update the shared state with the final answer to avoid\n",
    "    # polluting the dialogue history with intermediate messages\n",
    "    generated = await gen_answer_chain.ainvoke(swapped_state)\n",
    "    cited_urls = set(generated[\"parsed\"].cited_urls)\n",
    "    # Save the retrieved information to a the shared state for future reference\n",
    "    cited_references = {k: v for k, v in all_query_results.items() if k in cited_urls}\n",
    "    formatted_message = AIMessage(name=name, content=generated[\"parsed\"].as_str)\n",
    "    return {\"messages\": [formatted_message], \"references\": cited_references}\n",
    "\n",
    "\n",
    "builder = StateGraph(InterviewState)\n",
    "\n",
    "builder.add_node(\"ask_question\", generate_question, retry=RetryPolicy(max_attempts=5))\n",
    "builder.add_node(\"answer_question\", gen_answer, retry=RetryPolicy(max_attempts=5))\n",
    "builder.add_conditional_edges(\"answer_question\", route_messages)\n",
    "builder.add_edge(\"ask_question\", \"answer_question\")\n",
    "\n",
    "builder.add_edge(START, \"ask_question\")\n",
    "interview_graph = builder.compile(checkpointer=False).with_config(\n",
    "    run_name=\"Conduct Interviews\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812c21b5-1acd-448b-9feb-2c878ad6654c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ask_question\n",
      "--  [AIMessage(content=\"Yes, that's correct! As an AI Ethics Researcher, I'm particularly focused on exploring the ethical considerations and social implications of generative AI on daily life. To begin, could you share your thoughts on the potential privacy concerns that might arise as generative AI be\n"
     ]
    }
   ],
   "source": [
    "final_step = None\n",
    "\n",
    "initial_state = {\n",
    "    \"editor\": perspectives.editors[0],\n",
    "    \"messages\": [\n",
    "        AIMessage(\n",
    "            content=f\"So you said you were writing an article on {_input}?\",\n",
    "            name=\"Subject_Matter_Expert\",\n",
    "        )\n",
    "    ],\n",
    "}\n",
    "async for step in interview_graph.astream(initial_state):\n",
    "    name = next(iter(step))\n",
    "    print(name)\n",
    "    print(\"-- \", str(step[name][\"messages\"])[:300])\n",
    "final_step = step\n",
    "\n",
    "final_state = next(iter(final_step.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfac6ff-6daf-4c5d-9dfc-32edc64b4a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Refine Outline\n",
    "# Using turbo preview since the context can get quite long\n",
    "refine_outline_chain = refine_outline_prompt | long_context_llm.with_structured_output(\n",
    "    Outline\n",
    ")\n",
    "refined_outline = refine_outline_chain.invoke(\n",
    "    {\n",
    "        \"topic\": _input,\n",
    "        \"old_outline\": initial_outline.as_str,\n",
    "        \"conversations\": \"\\n\\n\".join(\n",
    "            f\"### {m.name}\\n\\n{m.content}\" for m in final_state[\"messages\"]\n",
    "        ),\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f9175c-90b7-4aca-9e15-5deacc12cf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "reference_docs = [\n",
    "    Document(page_content=v, metadata={\"source\": k})\n",
    "    for k, v in final_state[\"references\"].items()\n",
    "]\n",
    "# This really doesn't need to be a vectorstore for this size of data.\n",
    "# It could just be a numpy matrix. Or you could store documents\n",
    "# across requests if you want.\n",
    "vectorstore = InMemoryVectorStore.from_documents(\n",
    "    reference_docs,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "retriever = vectorstore.as_retriever(k=3)\n",
    "\n",
    "refined_outline = refine_outline_chain.invoke(\n",
    "    {\n",
    "        \"topic\": _input ,\n",
    "        \"old_outline\": initial_outline.as_str,\n",
    "        \"conversations\": \"\\n\\n\".join(\n",
    "            f\"### {m.name}\\n\\n{m.content}\" for m in final_state[\"messages\"]\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86fd25b-98b2-4257-832c-a2313a9982b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def retrieve(inputs: dict):\n",
    "    docs = await retriever.ainvoke(inputs[\"topic\"] + \": \" + inputs[\"section\"])\n",
    "    formatted = \"\\n\".join(\n",
    "        [\n",
    "            f'<Document href=\"{doc.metadata[\"source\"]}\"/>\\n{doc.page_content}\\n</Document>'\n",
    "            for doc in docs\n",
    "        ]\n",
    "    )\n",
    "    return {\"docs\": formatted, **inputs}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0e1a44-442c-411e-8987-63057225d145",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_writer = (\n",
    "    retrieve\n",
    "    | section_writer_prompt\n",
    "    | long_context_llm.with_structured_output(WikiSection)\n",
    ")\n",
    "\n",
    "section = await section_writer.ainvoke(\n",
    "    {\n",
    "        \"outline\": refined_outline.as_str,\n",
    "        \"section\": refined_outline.sections[1].section_title,\n",
    "        \"topic\": _input,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "writer = writer_prompt | long_context_llm | StrOutputParser()\n",
    "result = writer.invoke({\"topic\": _input, \"draft\": section.as_str})\n",
    "#print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adb5b41-c577-4056-9323-4f535396e1e5",
   "metadata": {},
   "source": [
    "# Out put "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda2e133-459e-4ff3-9e3c-ca4d82df580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in  refined_outline.sections :\n",
    "    print (i .section_title\n",
    "        \n",
    "    )\n",
    "    print( i.description)\n",
    "    print() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bb292f-2959-40c6-a185-caf9d7225241",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428878af-1f1f-4aac-811c-dc8b287d4364",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873a9bf3-9a1a-491e-b400-8927e67c7f97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9ef057-137c-4890-8922-264310a88f62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0810aaee-5d1e-4502-a649-a68e0db30014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6feefdc-7343-4aca-9441-1a41670e7210",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013891f5-73b3-4507-9a7f-56e368f8a101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88873814-425c-4b0a-8f88-defae8dffe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b136e2-bbfa-4ba6-b785-2c1b3cf8f9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b52aad-5ce0-4e73-b011-4cf18cded016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0b52cc-fced-44c8-b14c-8a833ed837ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a3219f-0a04-4dc3-80c0-a7313981f6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
